# Awesome-Human-Motion-Generation
[Single-Person Motion Generation](#motion-generation)
[Multi-Person Interaction Generation](#hhi) 

For more details on the research related to Human-Object Interaction, Human-Scene Interaction, and Human-Human Interaction, see https://github.com/Foruck/Awesome-Human-Motion.

<span id="motion-generation"></span>
<details open>
<summary><h2>Single-Person Motion Generation</h2></summary>
<ul style="margin-left: 5px;">
    <details open>
    <summary><h3>2025</h3></summary>
        <ul style="margin-left: 5px;">
        <li><b>(TOG 2025)</b> <a href="https://zhongleilz.github.io/Sketch2Anim/">Sketch2Anim</a>: Towards Transferring Sketch Storyboards into 3D Animation, Zhong et al.</li>
        <li><b>(SIGGRAPH 2025)</b> <a href="https://arxiv.org/pdf/2505.14087">Chang et al.</a>: Large-Scale Multi-Character Interaction Synthesis, Chang et al.</li>
        <li><b>(CVPR 2025)</b> <a href="https://arxiv.org/pdf/2505.00998">DSDFM</a>: Deterministic-to-Stochastic Diverse Latent Feature Mapping for Human Motion Synthesis, Hua et al.</li>
        <li><b>(CVPR 2025)</b> <a href="https://arxiv.org/abs/2504.05265">From Sparse Signal to Smooth Motion</a>: Real-Time Motion Generation with Rolling Prediction Models, Barquero et al.</li>
        <li><b>(CVPR 2025)</b> <a href="https://shape-move.github.io/">Shape My Moves</a>: Text-Driven Shape-Aware Synthesis of Human Motions, Liao et al.</li>
        <li><b>(CVPR 2025)</b> <a href="https://github.com/CVI-SZU/MG-MotionLLM">MG-MotionLLM</a>: A Unified Framework for Motion Comprehension and Generation across Multiple Granularities, Wu et al.</li>
        <li><b>(CVPR 2025)</b> <a href="https://seokhyeonhong.github.io/projects/salad/">SALAD</a>: Skeleton-aware Latent Diffusion for Text-driven Motion Generation and Editing, Hong et al.</li>
        <li><b>(CVPR 2025)</b> <a href="https://boeun-kim.github.io/page-PersonaBooth/">PersonalBooth</a>: Personalized Text-to-Motion Generation, Kim et al.</li>
        <li><b>(CVPR 2025)</b> <a href="https://arxiv.org/abs/2411.16575">MARDM</a>: Rethinking Diffusion for Text-Driven Human Motion Generation, Meng et al.</li>
        <li><b>(CVPR 2025)</b> <a href="https://arxiv.org/pdf/2503.04829">StickMotion</a>: Generating 3D Human Motions by Drawing a Stickman, Wang et al.</li>
        <li><b>(CVPR 2025)</b> <a href="https://arxiv.org/abs/2411.16805">LLaMo</a>: Human Motion Instruction Tuning, Li et al.</li>
        <li><b>(CVPR 2025)</b> <a href="https://star-uu-wang.github.io/HOP/">HOP</a>: Heterogeneous Topology-based Multimodal Entanglement for Co-Speech Gesture Generation, Cheng et al.</li>
        <li><b>(CVPR 2025)</b> <a href="https://atom-motion.github.io/">AtoM</a>: Aligning Text-to-Motion Model at Event-Level with GPT-4Vision Reward, Han et al.</li>
        <li><b>(CVPR 2025)</b> <a href="https://jiro-zhang.github.io/EnergyMoGen/">EnergyMoGen</a>: Compositional Human Motion Generation with Energy-Based Diffusion Model in Latent Space, Zhang et al.</li>
        <li><b>(CVPR 2025)</b> <a href="https://languageofmotion.github.io/">The Languate of Motion</a>: Unifying Verbal and Non-verbal Language of 3D Human Motion, Chen et al.</li>
        <li><b>(CVPR 2025)</b> <a href="https://shunlinlu.github.io/ScaMo/">ScaMo</a>: Exploring the Scaling Law in Autoregressive Motion Generation Model, Lu et al.</li>
        <li><b>(CVPR 2025)</b> <a href="https://hhsinping.github.io/Move-in-2D/">Move in 2D</a>: 2D-Conditioned Human Motion Generation, Huang et al.</li>
        <li><b>(CVPR 2025)</b> <a href="https://solami-ai.github.io/">SOLAMI</a>: Social Vision-Language-Action Modeling for Immersive Interaction with 3D Autonomous Characters, Jiang et al.</li>
        <li><b>(CVPR 2025)</b> <a href="https://lijiaman.github.io/projects/mvlift/">MVLift</a>: Lifting Motion to the 3D World via 2D Diffusion, Li et al.</li>
        <li><b>(CVPR 2025 Workshop)</b> <a href="https://arxiv.org/pdf/2505.10810">MoCLIP</a>: Motion-Aware Fine-Tuning and Distillation of CLIP for Human Motion Generation, Maldonado et al.</li>
        <li><b>(CVPR 2025 Workshop)</b> <a href="https://arxiv.org/pdf/2505.09827">Dyadic Mamba</a>: Long-term Dyadic Human Motion Synthesis, Tanke et al.</li>
        <li><b>(ACM Sensys 2025)</b> <a href="https://arxiv.org/pdf/2503.01768">SHADE-AD</a>: An LLM-Based Framework for Synthesizing Activity Data of Alzheimer’s Patients, Fu et al.</li>
        <li><b>(ICRA 2025)</b> <a href="https://arxiv.org/abs/2410.16623">MotionGlot</a>: A Multi-Embodied Motion Generation Model, Harithas et al.</li>
        <li><b>(ICLR 2025)</b> <a href="https://guytevet.github.io/CLoSD-page/">CLoSD</a>: Closing the Loop between Simulation and Diffusion for multi-task character control, Tevet et al.</li>
        <li><b>(ICLR 2025)</b> <a href="https://genforce.github.io/PedGen/">PedGen</a>: Learning to Generate Diverse Pedestrian Movements from Web Videos with Noisy Labels, Liu et al.</li>
        <li><b>(ICLR 2025)</b> <a href="https://openreview.net/forum?id=IEul1M5pyk">HGM³</a>: Hierarchical Generative Masked Motion Modeling with Hard Token Mining, Jeong et al.</li>
        <li><b>(ICLR 2025)</b> <a href="https://openreview.net/forum?id=LYawG8YkPa">LaMP</a>: Language-Motion Pretraining for Motion Generation, Retrieval, and Captioning, Li et al.</li>
        <li><b>(ICLR 2025)</b> <a href="https://openreview.net/forum?id=d23EVDRJ6g">MotionDreamer</a>: One-to-Many Motion Synthesis with Localized Generative Masked Transformer, Wang et al.</li>
        <li><b>(ICLR 2025)</b> <a href="https://openreview.net/forum?id=Oh8MuCacJW">Lyu et al</a>: Towards Unified Human Motion-Language Understanding via Sparse Interpretable Characterization, Lyu et al.</li>
        <li><b>(ICLR 2025)</b> <a href="https://zkf1997.github.io/DART/">DART</a>: A Diffusion-Based Autoregressive Motion Model for Real-Time Text-Driven Motion Control, Zhao et al.</li>
        <li><b>(ICLR 2025)</b> <a href="https://knoxzhao.github.io/Motion-Agent/">Motion-Agent</a>: A Conversational Framework for Human Motion Generation with LLMs, Wu et al.</li>
        <li><b>(IJCV 2025)</b> <a href="https://arxiv.org/pdf/2502.05534">Fg-T2M++</a>: LLMs-Augmented Fine-Grained Text Driven Human Motion Generation, Wang et al.</li>
        <li><b>(ArXiv 2025)</b> <a href="https://arxiv.org/pdf/2506.02452">ANT</a>: AdaptiveNeuralTemporal-Aware Text-to-Motion Model, Chen et al.</li>
        <li><b>(ArXiv 2025)</b> <a href="https://arxiv.org/pdf/2506.02661">MotionRAG-Diff</a>: A Retrieval-Augmented Diffusion Framework for Long-Term Music-to-Dance Generation, Huang et al.</li>
        <li><b>(ArXiv 2025)</b> <a href="https://arxiv.org/pdf/2505.21146">IKMo</a>: Image-Keyframed Motion Generation with Trajectory-Pose Conditioned Motion Diffusion Model, Zhao et al.</li>
        <li><b>(ArXiv 2025)</b> <a href="https://arxiv.org/pdf/2505.21531">Li et al</a>: How Much Do Large Language Models Know about Human Motion? A Case Study in 3D Avatar Control, Li et al.</li>
        <li><b>(ArXiv 2025)</b> <a href="https://arxiv.org/pdf/2505.21837">UniMoGen</a>: Universal Motion Generation, Khani et al.</li>
        <li><b>(ArXiv 2025)</b> <a href="https://arxiv.org/pdf/2505.23465">Wang et al</a>: Semantics-Aware Human Motion Generation from Audio Instructions, Wang et al.</li>
        <li><b>(ArXiv 2025)</b> <a href="https://arxiv.org/pdf/2505.19377">ACMDM</a>: Absolute Coordinates Make Motion Generation Easy, Meng et al.</li>
        <li><b>(ArXiv 2025)</b> <a href="https://mucunzhuzhu.github.io/PAMD-page/">PAMD</a>: Plausibility-Aware Motion Diffusion Model for Long Dance Generation, Zhu et al.</li>
        <li><b>(ArXiv 2025)</b> <a href="https://arxiv.org/pdf/2505.15197">Intentional Gesture</a>: Deliver Your Intentions with Gestures for Speech, Liu et al.</li>
        <li><b>(ArXiv 2025)</b> <a href="https://arxiv.org/pdf/2505.14222">MatchDance</a>: Collaborative Mamba-Transformer Architecture Matching for High-Quality 3D Dance Synthesis, Yang et al.</li>
        <li><b>(ArXiv 2025)</b> <a href="https://arxiv.org/pdf/2505.08293">M3G</a>: Multi-Granular Gesture Generator for Audio-Driven Full-Body Human Motion Synthesis, Yin et al.</li>
        <li><b>(ArXiv 2025)</b> <a href="https://arxiv.org/pdf/2505.05589">ReactDance</a>: Progressive-Granular Representation for Long-Term Coherent Reactive Dance Generation, Lin et al.</li>
        <li><b>(ArXiv 2025)</b> <a href="https://wengwanjiang.github.io/ReAlign-page/">ReAlign</a>: Bilingual Text-to-Motion Generation via Step-Aware Reward-Guided Alignment, Weng et al.</li>
        <li><b>(ArXiv 2025)</b> <a href="https://research.nvidia.com/labs/dair/genmo/">GENMO</a>: A GENeralist Model for Human MOtion, Li et al.</li>
        <li><b>(ArXiv 2025)</b> <a href="https://arxiv.org/pdf/2504.16722">ProMoGen</a>: PMG: Progressive Motion Generation via Sparse Anchor Postures Curriculum Learning, Xi et al.</li>
        <li><b>(ArXiv 2025)</b> <a href="https://xiangyuezhang.com/SemTalk/">SemTalk</a>: Holistic Co-speech Motion Generation with Frame-level Semantic Emphasis, Zhang et al.</li>
        <li><b>(ArXiv 2025)</b> <a href="https://foram-s1.github.io/DanceMosaic/">DanceMosaic</a>: High-Fidelity Dance Generation with Multimodal Editability, Shah et al.</li>
        <li><b>(ArXiv 2025)</b> <a href="https://yong-xie-xy.github.io/ReCoM/">ReCoM</a>: Realistic Co-Speech Motion Generation with Recurrent Embedded Transformer, Xie et al.</li>
        <li><b>(ArXiv 2025)</b> <a href="https://yz-cnsdqz.github.io/eigenmotion/PRIMAL/">PRIMAL</a>: Physically Reactive and Interactive Motor Model for Avatar Learning, Zhang et al.</li>
        <li><b>(ArXiv 2025)</b> <a href="https://www.pinlab.org/hmu">HMU</a>: Human Motion Unlearning, Matteis et al.</li>
        <li><b>(ArXiv 2025)</b> <a href="http://inwoohwang.me/SFControl">SFControl</a>: Motion Synthesis with Sparse and Flexible Keyjoint Control, Hwang et al.</li>
        <li><b>(ArXiv 2025)</b> <a href="https://arxiv.org/pdf/2503.14919">GenM3</a>: Generative Pretrained Multi-path Motion Model for Text Conditional Human Motion Generation, Shi et al.</li>
        <li><b>(ArXiv 2025)</b> <a href="https://zju3dv.github.io/MotionStreamer/">MotionStreamer</a>: Streaming Motion Generation via Diffusion-based Autoregressive Model in Causal Latent Space, Xiao et al.</li>
        <li><b>(ArXiv 2025)</b> <a href="https://arxiv.org/pdf/2503.13859">Less Is More</a>: Improving Motion Diffusion Models with Sparse Keyframes, Bae et al.</li>
        <li><b>(ArXiv 2025)</b> <a href="https://arxiv.org/pdf/2503.13300">Zeng et al</a>: Progressive Human Motion Generation Based on Text and Few Motion Frames, Zeng et al.</li>
        <li><b>(ArXiv 2025)</b> <a href="https://mjwei3d.github.io/ACMo/">ACMo</a>: Attribute Controllable Motion Generation, Wei et al.</li>
        <li><b>(ArXiv 2025)</b> <a href="https://jackyu6.github.io/HERO/">HERO</a>: Human Reaction Generation from Videos, Yu et al.</li>
        <li><b>(ArXiv 2025)</b> <a href="https://arxiv.org/pdf/2503.06151">BioMoDiffuse</a>: Physics-Guided Biomechanical Diffusion for Controllable and Authentic Human Motion Synthesis, Kang et al.</li>
        <li><b>(ArXiv 2025)</b> <a href="https://arxiv.org/pdf/2503.06499">ExGes</a>: Expressive Human Motion Retrieval and Modulation for Audio-Driven Gesture Synthesis, Zhou et al.</li>
        <li><b>(ArXiv 2025)</b> <a href="https://arxiv.org/pdf/2502.17327">AnyTop</a>: Character Animation Diffusion with Any Topology, Gat et al.</li>
        <li><b>(ArXiv 2025)</b> <a href="hhttps://steve-zeyu-zhang.github.io/MotionAnything/">Motion Anything</a>: Any to Motion Generation, Zhang et al.</li>
        <li><b>(ArXiv 2025)</b> <a href="https://arxiv.org/pdf/2502.18309">GCDance</a>: Genre-Controlled 3D Full Body Dance Generation Driven By Music, Liu et al.</li>
        <li><b>(ArXiv 2025)</b> <a href="https://diouo.github.io/motionlab.github.io/">MotionLab</a>: Unified Human Motion Generation and Editing via the Motion-Condition-Motion Paradigm, Guo et al.</li>
        <li><b>(ArXiv 2025)</b> <a href="https://cjerry1243.github.io/casim_t2m/">CASIM</a>: Composite Aware Semantic Injection for Text to Motion Generation, Chang et al.</li>
        <li><b>(ArXiv 2025)</b> <a href="https://arxiv.org/pdf/2501.19083">MotionPCM</a>: Real-Time Motion Synthesis with Phased Consistency Model, Jiang et al.</li>
        <li><b>(ArXiv 2025)</b> <a href="https://andypinxinliu.github.io/GestureLSM/">GestureLSM</a>: Latent Shortcut based Co-Speech Gesture Generation with Spatial-Temporal Modeling, Liu et al.</li>
        <li><b>(ArXiv 2025)</b> <a href="https://arxiv.org/pdf/2501.18232">Free-T2M</a>: Frequency Enhanced Text-to-Motion Diffusion Model With Consistency Loss, Chen et al.</li>
        <li><b>(ArXiv 2025)</b> <a href="https://arxiv.org/pdf/2501.01449">LS-GAN</a>: Human Motion Synthesis with Latent-space GANs, Amballa et al.</li>
        <li><b>(ArXiv 2025)</b> <a href="https://arxiv.org/html/2501.16778v1">FlexMotion</a>: Lightweight, Physics-Aware, and Controllable Human Motion Generation, Tashakori et al.</li>
        <li><b>(ArXiv 2025)</b> <a href="https://arxiv.org/pdf/2503.06897">HiSTF Mamba</a>: Hierarchical Spatiotemporal Fusion with Multi-Granular Body-Spatial Modeling for High-Fidelity Text-to-Motion Generation, Zhan et al.</li>
        <li><b>(ArXiv 2025)</b> <a href="https://arxiv.org/pdf/2501.16551">PackDiT</a>: Joint Human Motion and Text Generation via Mutual Prompting, Jiang et al.</li>
        <li><b>(3DV 2025)</b> <a href="https://coral79.github.io/uni-motion/">Unimotion</a>: Unifying 3D Human Motion Synthesis and Understanding, Li et al.</li>
        <li><b>(3DV 2025)</b> <a href="https://cyk990422.github.io/HoloGest.github.io//">HoloGest</a>: Decoupled Diffusion and Motion Priors for Generating Holisticly Expressive Co-speech Gestures, Cheng et al.</li>
        <li><b>(AAAI 2025)</b> <a href="https://hanyangclarence.github.io/unimumo_demo/">UniMuMo</a>: Unified Text, Music and Motion Generation, Yang et al.</li>
        <li><b>(AAAI 2025)</b> <a href="https://arxiv.org/abs/2408.00352">ALERT-Motion</a>: Autonomous LLM-Enhanced Adversarial Attack for Text-to-Motion, Miao et al.</li>
        <li><b>(AAAI 2025)</b> <a href="https://cure-lab.github.io/MotionCraft/">MotionCraft</a>: Crafting Whole-Body Motion with Plug-and-Play Multimodal Controls, Bian et al.</li>
        <li><b>(AAAI 2025)</b> <a href="https://arxiv.org/pdf/2412.11193">Light-T2M</a>: A Lightweight and Fast Model for Text-to-Motion Generation, Zeng et al.</li>
        <li><b>(WACV 2025)</b> <a href="https://reindiffuse.github.io/">ReinDiffuse</a>: Crafting Physically Plausible Motions with Reinforced Diffusion Model, Han et al.</li>
        <li><b>(WACV 2025)</b> <a href="https://motion-rag.github.io/">MoRAG</a>: Multi-Fusion Retrieval Augmented Generation for Human Motion, Shashank et al.</li>
        <li><b>(WACV 2025)</b> <a href="https://arxiv.org/abs/2409.11920">Mandelli et al</a>: Generation of Complex 3D Human Motion by Temporal and Spatial Composition of Diffusion Models, Mandelli et al.</li>
    </ul></details>
    <details>
    <summary><h3>2024</h3></summary>
        <ul style="margin-left: 5px;">
        <li><b>(ArXiv 2024)</b> <a href="https://xiangyue-zhang.github.io/SemTalk">SemTalk</a>: Holistic Co-speech Motion Generation with Frame-level Semantic Emphasis, Zhang et al.</li>
        <li><b>(ArXiv 2024)</b> <a href="https://inter-dance.github.io/">InterDance</a>: Reactive 3D Dance Generation with Realistic Duet Interactions, Li et al.</li>
        <li><b>(ArXiv 2024)</b> <a href="https://zju3dv.github.io/Motion-2-to-3/">Motion-2-to-3</a>: Leveraging 2D Motion Data to Boost 3D Motion Generation, Pi et al.</li>
        <li><b>(ArXiv 2024)</b> <a href="https://arxiv.org/abs/2412.07797">Mogo</a>: RQ Hierarchical Causal Transformer for High-Quality 3D Human Motion Generation, Fu et al.</li>
        <li><b>(ArXiv 2024)</b> <a href="https://gabrie-l.github.io/coma-page/">CoMA</a>: Compositional Human Motion Generation with Multi-modal Agents, Sun et al.</li>
        <li><b>(ArXiv 2024)</b> <a href="https://sopo-motion.github.io/">SoPo</a>: Text-to-Motion Generation Using Semi-Online Preference Optimization, Tan et al.</li>
        <li><b>(ArXiv 2024)</b> <a href="https://arxiv.org/pdf/2412.04343">RMD</a>: A Simple Baseline for More General Human Motion Generation via Training-free Retrieval-Augmented Motion Diffuse, Liao et al.</li>
        <li><b>(ArXiv 2024)</b> <a href="https://arxiv.org/pdf/2412.00112">BiPO</a>: Bidirectional Partial Occlusion Network for Text-to-Motion Synthesis, Hong et al.</li>
        <li><b>(ArXiv 2024)</b> <a href="https://whwjdqls.github.io/discord.github.io/">DisCoRD</a>: Discrete Tokens to Continuous Motion via Rectified Flow Decoding, Cho et al.</li>
        <li><b>(ArXiv 2024)</b> <a href="https://arxiv.org/abs/2411.19786">MoTe</a>: Learning Motion-Text Diffusion Model for Multiple Generation Tasks, Wue et al.</li>
        <li><b>(ArXiv 2024)</b> <a href="https://arxiv.org/abs/2411.18303">InfiniDreamer</a>: Arbitrarily Long Human Motion Generation via Segment Score Distillation, Zhuo et al.</li>
        <li><b>(ArXiv 2024)</b> <a href="https://arxiv.org/abs/2411.17532">FTMoMamba</a>: Motion Generation with Frequency and Text State Space Models, Li et al.</li>
        <li><b>(ArXiv 2024)</b> <a href="https://andypinxinliu.github.io/KinMo/">KinMo</a>: Kinematic-aware Human Motion Understanding and Generation, Zhang et al.</li>
        <li><b>(ArXiv 2024)</b> <a href="https://arxiv.org/abs/2411.14951">Morph</a>: A Motion-free Physics Optimization Framework for Human Motion Generation, Li et al.</li>
        <li><b>(ArXiv 2024)</b> <a href="https://steve-zeyu-zhang.github.io/KMM">KMM</a>: Key Frame Mask Mamba for Extended Motion Generation, Zhang et al.</li>
        <li><b>(ArXiv 2024)</b> <a href="https://arxiv.org/abs/2410.21747">MotionGPT-2</a>: A General-Purpose Motion-Language Model for Motion Generation and Understanding, Wang et al.</li>
        <li><b>(ArXiv 2024)</b> <a href="https://li-ronghui.github.io/lodgepp">Lodge++</a>: High-quality and Long Dance Generation with Vivid Choreography Patterns, Li et al.</li>
        <li><b>(ArXiv 2024)</b> <a href="https://arxiv.org/abs/2410.18977">MotionCLR</a>: Motion Generation and Training-Free Editing via Understanding Attention Mechanisms, Chen et al.</li>
        <li><b>(ArXiv 2024)</b> <a href="https://arxiv.org/abs/2410.14508">LEAD</a>: Latent Realignment for Human Motion Diffusion, Andreou et al.</li>
        <li><b>(ArXiv 2024)</b> <a href="https://arxiv.org/abs/2410.08931">Leite et al.</a> Enhancing Motion Variation in Text-to-Motion Models via Pose and Video Conditioned Editing, Leite et al.</li>
        <li><b>(ArXiv 2024)</b> <a href="https://arxiv.org/abs/2410.06513">MotionRL</a>: Align Text-to-Motion Generation to Human Preferences with Multi-Reward Reinforcement Learning, Liu et al.</li>
        <li><b>(ArXiv 2024)</b> <a href="https://lhchen.top/MotionLLM/">MotionLLM</a>: Understanding Human Behaviors from Human Motions and Videos, Chen et al.</li>
        <li><b>(ArXiv 2024)</b> <a href="https://arxiv.org/abs/2410.03311">Wang et al</a>. Quo Vadis, Motion Generation? From Large Language Models to Large Motion Models, Wang et al.</li>
        <li><b>(ArXiv 2024)</b> <a href="https://arxiv.org/abs/2409.13251">T2M-X</a>: Learning Expressive Text-to-Motion Generation from Partially Annotated Data, Liu et al.</li>
        <li><b>(ArXiv 2024)</b> <a href="https://github.com/RohollahHS/BAD">BAD</a>: Bidirectional Auto-regressive Diffusion for Text-to-Motion Generation, Hosseyni et al.</li>
        <li><b>(ArXiv 2024)</b> <a href="https://von31.github.io/synNsync/">synNsync</a>: Synergy and Synchrony in Couple Dances, Manukele et al.</li>
        <li><b>(EMNLP 2024)</b> <a href="https://aclanthology.org/2024.findings-emnlp.584/">Dong et al</a>: Word-Conditioned 3D American Sign Language Motion Generation, Dong et al.</li>
        <li><b>(NeurIPS D&B 2024)</b> <a href="https://nips.cc/virtual/2024/poster/97700">Kim et al</a>: Text to Blind Motion, Kim et al.</li>
        <li><b>(NeurIPS 2024)</b> <a href="https://github.com/xiyuanzh/UniMTS">UniMTS</a>: Unified Pre-training for Motion Time Series, Zhang et al.</li>
        <li><b>(NeurIPS 2024)</b> <a href="https://openreview.net/forum?id=FsdB3I9Y24">Christopher et al.</a>: Constrained Synthesis with Projected Diffusion Models, Christopher et al.</li>
        <li><b>(NeurIPS 2024)</b> <a href="https://momu-diffusion.github.io/">MoMu-Diffusion</a>: On Learning Long-Term Motion-Music Synchronization and Correspondence, You et al.</li>
        <li><b>(NeurIPS 2024)</b> <a href="https://aigc3d.github.io/mogents/">MoGenTS</a>: Motion Generation based on Spatial-Temporal Joint Modeling, Yuan et al.</li>
        <li><b>(NeurIPS 2024)</b> <a href="https://arxiv.org/abs/2405.16273">M3GPT</a>: An Advanced Multimodal, Multitask Framework for Motion Comprehension and Generation, Luo et al.</li>
        <li><b>(NeurIPS Workshop 2024)</b> <a href="https://openreview.net/forum?id=BTSnh5YdeI">Bikov et al</a>: Fitness Aware Human Motion Generation with Fine-Tuning, Bikov et al.</li>
        <li><b>(NeurIPS Workshop 2024)</b> <a href="https://arxiv.org/pdf/2502.20176">DGFM</a>: Full Body Dance Generation Driven by Music Foundation Models, Liu et al.</li>
        <li><b>(ICPR 2024)</b> <a href="https://link.springer.com/chapter/10.1007/978-3-031-78104-9_30">FG-MDM</a>: Towards Zero-Shot Human Motion Generation via ChatGPT-Refined Descriptions, Shi et al.</li>
        <li><b>(ACM MM 2024)</b> <a href="https://bohongchen.github.io/SynTalker-Page/">SynTalker</a>: Enabling Synergistic Full-Body Control in Prompt-Based Co-Speech Motion Generation, Chen et al.</li>
        <li><b>(ACM MM 2024)</b> <a href="https://dl.acm.org/doi/abs/10.1145/3664647.3681487">L3EM</a>: Towards Emotion-enriched Text-to-Motion Generation via LLM-guided Limb-level Emotion Manipulating. Yu et al.</li>
        <li><b>(ACM MM 2024)</b> <a href="https://dl.acm.org/doi/abs/10.1145/3664647.3681657">StableMoFusion</a>: Towards Robust and Efficient Diffusion-based Motion Generation Framework, Huang et al.</li>
        <li><b>(ACM MM 2024)</b> <a href="https://dl.acm.org/doi/abs/10.1145/3664647.3681034">SATO</a>: Stable Text-to-Motion Framework, Chen et al.</li>
        <li><b>(ICANN 2024)</b> <a href="https://link.springer.com/chapter/10.1007/978-3-031-72356-8_2">PIDM</a>: Personality-Aware Interaction Diffusion Model for Gesture Generation, Shibasaki et al.</li>
        <li><b>(HFES 2024)</b> <a href="https://journals.sagepub.com/doi/full/10.1177/10711813241262026">Macwan et al</a>: High-Fidelity Worker Motion Simulation With Generative AI, Macwan et al.</li>
        <li><b>(ECCV 2024)</b> <a href="https://jpthu17.github.io/GuidedMotion-project/">Jin et al</a>: Local Action-Guided Motion Diffusion Model for Text-to-Motion Generation, Jin et al.</li>
        <li><b>(ECCV 2024)</b> <a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/100_ECCV_2024_paper.php">Motion Mamba</a>: Efficient and Long Sequence Motion Generation, Zhong et al.</li>
        <li><b>(ECCV 2024)</b> <a href="https://frank-zy-dou.github.io/projects/EMDM/index.html">EMDM</a>: Efficient Motion Diffusion Model for Fast, High-Quality Human Motion Generation, Zhou et al.</li>
        <li><b>(ECCV 2024)</b> <a href="https://yh2371.github.io/como/">CoMo</a>: Controllable Motion Generation through Language Guided Pose Code Editing, Huang et al.</li>
        <li><b>(ECCV 2024)</b> <a href="https://github.com/jsun57/CoMusion">CoMusion</a>: Towards Consistent Stochastic Human Motion Prediction via Motion Diffusion, Sun et al.</li>
        <li><b>(ECCV 2024)</b> <a href="https://arxiv.org/abs/2405.18483">Shan et al</a>: Towards Open Domain Text-Driven Synthesis of Multi-Person Motions, Shan et al.</li>
        <li><b>(ECCV 2024)</b> <a href="https://github.com/qrzou/ParCo">ParCo</a>: Part-Coordinating Text-to-Motion Synthesis, Zou et al.</li>
        <li><b>(ECCV 2024)</b> <a href="https://arxiv.org/abs/2407.11532">Sampieri et al</a>: Length-Aware Motion Synthesis via Latent Diffusion, Sampieri et al.</li>
        <li><b>(ECCV 2024)</b> <a href="https://github.com/line/ChronAccRet">ChroAccRet</a>: Chronologically Accurate Retrieval for Temporal Grounding of Motion-Language Models, Fujiwara et al.</li>
        <li><b>(ECCV 2024)</b> <a href="https://idigitopia.github.io/projects/mhc/">MHC</a>: Generating Physically Realistic and Directable Human Motions from Multi-Modal Inputs, Liu et al.</li>
        <li><b>(ECCV 2024)</b> <a href="https://github.com/moonsliu/Pro-Motion">ProMotion</a>: Plan, Posture and Go: Towards Open-vocabulary Text-to-Motion Generation, Liu et al.</li>
        <li><b>(ECCV 2024)</b> <a href="https://arxiv.org/abs/2406.10740">FreeMotion</a>: MoCap-Free Human Motion Synthesis with Multimodal Large Language Models, Zhang et al.</li>
        <li><b>(ECCV 2024)</b> <a href="https://eccv.ecva.net/virtual/2024/poster/266">Text Motion Translator</a>: A Bi-Directional Model for Enhanced 3D Human Motion Generation from Open-Vocabulary Descriptions, Qian et al.</li>
        <li><b>(ECCV 2024)</b> <a href="https://vankouf.github.io/FreeMotion/">FreeMotion</a>: A Unified Framework for Number-free Text-to-Motion Synthesis, Fan et al.</li>
        <li><b>(ECCV 2024)</b> <a href="https://foruck.github.io/KP/">Kinematic Phrases</a>: Bridging the Gap between Human Motion and Action Semantics via Kinematic Phrases, Liu et al.</li>
        <li><b>(ECCV 2024)</b> <a href="https://arxiv.org/abs/2404.01700">MotionChain</a>: Conversational Motion Controllers via Multimodal Prompts, Jiang et al.</li>
        <li><b>(ECCV 2024)</b> <a href="https://neu-vi.github.io/SMooDi/">SMooDi</a>: Stylized Motion Diffusion Model, Zhong et al.</li>
        <li><b>(ECCV 2024)</b> <a href="https://exitudio.github.io/BAMM-page/">BAMM</a>: Bidirectional Autoregressive Motion Model, Pinyoanuntapong et al.</li>
        <li><b>(ECCV 2024)</b> <a href="https://dai-wenxun.github.io/MotionLCM-page/">MotionLCM</a>: Real-time Controllable Motion Generation via Latent Consistency Model, Dai et al.</li>
        <li><b>(ECCV 2024)</b> <a href="https://arxiv.org/abs/2312.10993">Ren et al</a>: Realistic Human Motion Generation with Cross-Diffusion Models, Ren et al.</li>
        <li><b>(ECCV 2024)</b> <a href="https://arxiv.org/abs/2407.14502">M2D2M</a>: Multi-Motion Generation from Text with Discrete Diffusion Models, Chi et al.</li>
        <li><b>(ECCV 2024)</b> <a href="https://mingyuan-zhang.github.io/projects/LMM.html">Large Motion Model</a>: Large Motion Model for Unified Multi-Modal Motion Generation, Zhang et al.</li>
        <li><b>(ECCV 2024)</b> <a href="https://research.nvidia.com/labs/toronto-ai/tesmo/">TesMo</a>: Generating Human Interaction Motions in Scenes with Text Control, Yi et al.</li>
        <li><b>(ECCV 2024)</b> <a href="https://tlcontrol.weilinwl.com/">TLcontrol</a>: Trajectory and Language Control for Human Motion Synthesis, Wan et al.</li>
        <li><b>(ICME 2024)</b> <a href="https://ieeexplore.ieee.org/abstract/document/10687922">ExpGest</a>: Expressive Speaker Generation Using Diffusion Model and Hybrid Audio-Text Guidance, Cheng et al.</li>
        <li><b>(ICME Workshop 2024)</b> <a href="https://ieeexplore.ieee.org/abstract/document/10645445">Chen et al</a>: Anatomically-Informed Vector Quantization Variational Auto-Encoder for Text-to-Motion Generation, Chen et al.</li>
        <li><b>(ICML 2024)</b> <a href="https://github.com/LinghaoChan/HumanTOMATO">HumanTOMATO</a>: Text-aligned Whole-body Motion Generation, Lu et al.</li>
        <li><b>(ICML 2024)</b> <a href="https://sites.google.com/view/gphlvm/">GPHLVM</a>: Bringing Motion Taxonomies to Continuous Domains via GPLVM on Hyperbolic Manifolds, Jaquier et al.</li>
        <li><b>(SIGGRAPH 2024)</b> <a href="https://setarehc.github.io/CondMDI/">CondMDI</a>: Flexible Motion In-betweening with Diffusion Models, Cohan et al.</li>
        <li><b>(SIGGRAPH 2024)</b> <a href="https://aiganimation.github.io/CAMDM/">CAMDM</a>: Taming Diffusion Probabilistic Models for Character Control, Chen et al.</li>
        <li><b>(SIGGRAPH 2024)</b> <a href="https://vcc.tech/research/2024/LGTM">LGTM</a>: Local-to-Global Text-Driven Human Motion Diffusion Models, Sun et al.</li>
        <li><b>(SIGGRAPH 2024)</b> <a href="https://threedle.github.io/TEDi/">TEDi</a>: Temporally-Entangled Diffusion for Long-Term Motion Synthesis, Zhang et al.</li>
        <li><b>(SIGGRAPH 2024)</b> <a href="https://github.com/Yi-Shi94/AMDM">A-MDM</a>: Interactive Character Control with Auto-Regressive Motion Diffusion Models, Shi et al.</li>
        <li><b>(SIGGRAPH 2024)</b> <a href="https://dl.acm.org/doi/10.1145/3658209">Starke et al</a>: Categorical Codebook Matching for Embodied Character Controllers, Starke et al.</li>
        <li><b>(SIGGRAPH 2024)</b> <a href="https://arxiv.org/abs/2407.10481">SuperPADL</a>: Scaling Language-Directed Physics-Based Control with Progressive Supervised Distillation, Juravsky et al.</li>
        <li><b>(CVPR 2024)</b> <a href="https://hanchaoliu.github.io/Prog-MoGen/">ProgMoGen</a>: Programmable Motion Generation for Open-set Motion Control Tasks, Liu et al.</li>
        <li><b>(CVPR 2024)</b> <a href="https://github.com/IDC-Flash/PacerPlus">PACER+</a>: On-Demand Pedestrian Animation Controller in Driving Scenarios, Wang et al.</li>
        <li><b>(CVPR 2024)</b> <a href="https://amuse.is.tue.mpg.de/">AMUSE</a>: Emotional Speech-driven 3D Body Animation via Disentangled Latent Diffusion, Chhatre et al.</li>
        <li><b>(CVPR 2024)</b> <a href="https://feifeifeiliu.github.io/probtalk/">Liu et al</a>: Towards Variable and Coordinated Holistic Co-Speech Motion Generation, Liu et al.</li>
        <li><b>(CVPR 2024)</b> <a href="https://guytevet.github.io/mas-page/">MAS</a>: Multi-view Ancestral Sampling for 3D motion generation using 2D diffusion, Kapon et al.</li>
        <li><b>(CVPR 2024)</b> <a href="https://wandr.is.tue.mpg.de/">WANDR</a>: Intention-guided Human Motion Generation, Diomataris et al.</li>
        <li><b>(CVPR 2024)</b> <a href="https://ericguo5513.github.io/momask/">MoMask</a>: Generative Masked Modeling of 3D Human Motions, Guo et al.</li>
        <li><b>(CVPR 2024)</b> <a href="https://yfeng95.github.io/ChatPose/">ChapPose</a>: Chatting about 3D Human Pose, Feng et al.</li>
        <li><b>(CVPR 2024)</b> <a href="https://zixiangzhou916.github.io/AvatarGPT/">AvatarGPT</a>: All-in-One Framework for Motion Understanding, Planning, Generation and Beyond, Zhou et al.</li>
        <li><b>(CVPR 2024)</b> <a href="https://exitudio.github.io/MMM-page/">MMM</a>: Generative Masked Motion Model, Pinyoanuntapong et al.</li>
        <li><b>(CVPR 2024)</b> <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Li_AAMDM_Accelerated_Auto-regressive_Motion_Diffusion_Model_CVPR_2024_paper.pdf">AAMDM</a>: Accelerated Auto-regressive Motion Diffusion Model, Li et al.</li>
        <li><b>(CVPR 2024)</b> <a href="https://tr3e.github.io/omg-page/">OMG</a>: Towards Open-vocabulary Motion Generation via Mixture of Controllers, Liang et al.</li>
        <li><b>(CVPR 2024)</b> <a href="https://barquerogerman.github.io/FlowMDM/">FlowMDM</a>: Seamless Human Motion Composition with Blended Positional Encodings, Barquero et al.</li>
        <li><b>(CVPR 2024)</b> <a href="https://digital-life-project.com/">Digital Life Project</a>: Autonomous 3D Characters with Social Intelligence, Cai et al.</li>
        <li><b>(CVPR 2024)</b> <a href="https://pantomatrix.github.io/EMAGE/">EMAGE</a>: Towards Unified Holistic Co-Speech Gesture Generation via Expressive Masked Audio Gesture Modeling, Liu et al.</li>
        <li><b>(CVPR Workshop 2024)</b> <a href="https://xbpeng.github.io/projects/STMC/index.html">STMC</a>: Multi-Track Timeline Control for Text-Driven 3D Human Motion Generation, Petrovich et al.</li>
        <li><b>(CVPR Workshop 2024)</b> <a href="https://github.com/THU-LYJ-Lab/InstructMotion">InstructMotion</a>: Exploring Text-to-Motion Generation with Human Preference, Sheng et al.</li>
        <li><b>(ICLR 2024)</b> <a href="https://sinmdm.github.io/SinMDM-page/">Single Motion Diffusion</a>: Raab et al.</li>
        <li><b>(ICLR 2024)</b> <a href="https://openreview.net/forum?id=sOJriBlOFd&noteId=KaJUBoveeo">NeRM</a>: Learning Neural Representations for High-Framerate Human Motion Synthesis, Wei et al.</li>
        <li><b>(ICLR 2024)</b> <a href="https://priormdm.github.io/priorMDM-page/">PriorMDM</a>: Human Motion Diffusion as a Generative Prior, Shafir et al.</li>
        <li><b>(ICLR 2024)</b> <a href="https://neu-vi.github.io/omnicontrol/">OmniControl</a>: Control Any Joint at Any Time for Human Motion Generation, Xie et al.</li>
        <li><b>(ICLR 2024)</b> <a href="https://openreview.net/forum?id=yQDFsuG9HP">Adiya et al.</a>: Bidirectional Temporal Diffusion Model for Temporally Consistent Human Animation, Adiya et al.</li>
        <li><b>(ICLR 2024)</b> <a href="https://lisiyao21.github.io/projects/Duolando/">Duolando</a>: Follower GPT with Off-Policy Reinforcement Learning for Dance Accompaniment, Li et al.</li>
        <li><b>(AAAI 2024)</b> <a href="https://arxiv.org/abs/2312.12227">HuTuDiffusion</a>: Human-Tuned Navigation of Latent Motion Diffusion Models with Minimal Feedback, Han et al.</li>
        <li><b>(AAAI 2024)</b> <a href="https://arxiv.org/abs/2312.12763">AMD</a>: Anatomical Motion Diffusion with Interpretable Motion Decomposition and Fusion, Jing et al.</li>
        <li><b>(AAAI 2024)</b> <a href="https://nhathoang2002.github.io/MotionMix-page/">MotionMix</a>: Weakly-Supervised Diffusion for Controllable Motion Generation, Hoang et al.</li>
        <li><b>(AAAI 2024)</b> <a href="https://github.com/xiezhy6/B2A-HDM">B2A-HDM</a>: Towards Detailed Text-to-Motion Synthesis via Basic-to-Advanced Hierarchical Diffusion Model, Xie et al.</li>
        <li><b>(AAAI 2024)</b> <a href="https://ojs.aaai.org/index.php/AAAI/article/view/27936">Everything2Motion</a>: Everything2Motion: Synchronizing Diverse Inputs via a Unified Framework for Human Motion Synthesis, Fan et al.</li>
        <li><b>(AAAI 2024)</b> <a href="https://qiqiapink.github.io/MotionGPT/">MotionGPT</a>: Finetuned LLMs are General-Purpose Motion Generators, Zhang et al.</li>
        <li><b>(AAAI 2024)</b> <a href="https://arxiv.org/abs/2305.13773">Dong et al</a>: Enhanced Fine-grained Motion Diffusion for Text-driven Human Motion Synthesis, Dong et al.</li>
        <li><b>(AAAI 2024)</b> <a href="https://evm7.github.io/UNIMASKM-page/">UNIMASKM</a>: A Unified Masked Autoencoder with Patchified Skeletons for Motion Synthesis, Mascaro et al.</li>
        <li><b>(AAAI 2024)</b> <a href="https://arxiv.org/abs/2312.10960">B2A-HDM</a>: Towards Detailed Text-to-Motion Synthesis via Basic-to-Advanced Hierarchical Diffusion Model, Xie et al.</li>
        <li><b>(TPAMI 2024)</b> <a href="https://ieeexplore.ieee.org/abstract/document/10399852">GUESS</a>: GradUally Enriching SyntheSis for Text-Driven Human Motion Generation, Gao et al.</li>
        <li><b>(WACV 2024)</b> <a href="https://arxiv.org/pdf/2312.12917">Xie et al.</a>: Sign Language Production with Latent Motion Transformer, Xie et al.</li>
    </ul></details>
    <details>
    <summary><h3>2023</h3></summary>
    <ul style="margin-left: 5px;">
        <li><b>(NeurIPS 2023)</b> <a href="https://github.com/jpthu17/GraphMotion">GraphMotion</a>: Act As You Wish: Fine-grained Control of Motion Diffusion Model with Hierarchical Semantic Graphs, Jin et al.</li>
        <li><b>(NeurIPS 2023)</b> <a href="https://motion-gpt.github.io/">MotionGPT</a>: Human Motion as Foreign Language, Jiang et al.</li>
        <li><b>(NeurIPS 2023)</b> <a href="https://mingyuan-zhang.github.io/projects/FineMoGen.html">FineMoGen</a>: Fine-Grained Spatio-Temporal Motion Generation and Editing, Zhang et al.</li>
        <li><b>(NeurIPS 2023)</b> <a href="https://jiawei-ren.github.io/projects/insactor/">InsActor</a>: Instruction-driven Physics-based Characters, Ren et al.</li>
        <li><b>(ICCV 2023)</b> <a href="https://github.com/ZcyMonkey/AttT2M">AttT2M</a>: Text-Driven Human Motion Generation with Multi-Perspective Attention Mechanism, Zhong et al.</li>
        <li><b>(ICCV 2023)</b> <a href="https://mathis.petrovich.fr/tmr">TMR</a>: Text-to-Motion Retrieval Using Contrastive 3D Human Motion Synthesis, Petrovich et al.</li>
        <li><b>(ICCV 2023)</b> <a href="https://azadis.github.io/make-an-animation">MAA</a>: Make-An-Animation: Large-Scale Text-conditional 3D Human Motion Generation, Azadi et al.</li>
        <li><b>(ICCV 2023)</b> <a href="https://nvlabs.github.io/PhysDiff">PhysDiff</a>: Physics-Guided Human Motion Diffusion Model, Yuan et al.</li>
        <li><b>(ICCV 2023)</b> <a href="https://mingyuan-zhang.github.io/projects/ReMoDiffuse.html">ReMoDiffuse</a>: Retrieval-Augmented Motion Diffusion Model, Zhang et al.</li>
        <li><b>(ICCV 2023)</b> <a href="https://barquerogerman.github.io/BeLFusion/">BelFusion</a>: Latent Diffusion for Behavior-Driven Human Motion Prediction, Barquero et al.</li>
        <li><b>(ICCV 2023)</b> <a href="https://korrawe.github.io/gmd-project/">GMD</a>: Guided Motion Diffusion for Controllable Human Motion Synthesis, Karunratanakul et al.</li>
        <li><b>(ICCV 2023)</b> <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Aliakbarian_HMD-NeMo_Online_3D_Avatar_Motion_Generation_From_Sparse_Observations_ICCV_2023_paper.html">HMD-NeMo</a>: Online 3D Avatar Motion Generation From Sparse Observations, Aliakbarian et al.</li>
        <li><b>(ICCV 2023)</b> <a href="https://sinc.is.tue.mpg.de/">SINC</a>: Spatial Composition of 3D Human Motions for Simultaneous Action Generation, Athanasiou et al.</li>
        <li><b>(ICCV 2023)</b> <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Kong_Priority-Centric_Human_Motion_Generation_in_Discrete_Latent_Space_ICCV_2023_paper.html">Kong et al.</a>: Priority-Centric Human Motion Generation in Discrete Latent Space, Kong et al.</li>
        <li><b>(ICCV 2023)</b> <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Wang_Fg-T2M_Fine-Grained_Text-Driven_Human_Motion_Generation_via_Diffusion_Model_ICCV_2023_paper.html">Fg-T2M</a>: Fine-Grained Text-Driven Human Motion Generation via Diffusion Model, Wang et al.</li>
        <li><b>(ICCV 2023)</b> <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Qian_Breaking_The_Limits_of_Text-conditioned_3D_Motion_Synthesis_with_Elaborative_ICCV_2023_paper.html">EMS</a>: Breaking The Limits of Text-conditioned 3D Motion Synthesis with Elaborative Descriptions, Qian et al.</li>
        <li><b>(SIGGRAPH 2023)</b> <a href="https://weiyuli.xyz/GenMM/">GenMM</a>: Example-based Motion Synthesis via Generative Motion Matching, Li et al.</li>
        <li><b>(SIGGRAPH 2023)</b> <a href="https://pku-mocca.github.io/GestureDiffuCLIP-Page/">GestureDiffuCLIP</a>: Gesture Diffusion Model with CLIP Latents, Ao et al.</li>
        <li><b>(SIGGRAPH 2023)</b> <a href="https://i.cs.hku.hk/~taku/kunkun2023.pdf">BodyFormer</a>: Semantics-guided 3D Body Gesture Synthesis with Transformer, Pang et al.</li>
        <li><b>(SIGGRAPH 2023)</b> <a href="https://www.speech.kth.se/research/listen-denoise-action/">Alexanderson et al.</a>: Listen, denoise, action! Audio-driven motion synthesis with diffusion models, Alexanderson et al.</li>
        <li><b>(CVPR 2023)</b> <a href="https://dulucas.github.io/agrol/">AGroL</a>: Avatars Grow Legs: Generating Smooth Human Motion from Sparse Tracking Inputs with Diffusion Model, Du et al.</li>
        <li><b>(CVPR 2023)</b> <a href="https://talkshow.is.tue.mpg.de/">TALKSHOW</a>: Generating Holistic 3D Human Motion from Speech, Yi et al.</li>
        <li><b>(CVPR 2023)</b> <a href="https://mael-zys.github.io/T2M-GPT/">T2M-GPT</a>: Generating Human Motion from Textual Descriptions with Discrete Representations, Zhang et al.</li>
        <li><b>(CVPR 2023)</b> <a href="https://zixiangzhou916.github.io/UDE/">UDE</a>: A Unified Driving Engine for Human Motion Generation, Zhou et al.</li>
        <li><b>(CVPR 2023)</b> <a href="https://github.com/junfanlin/oohmg">OOHMG</a>: Being Comes from Not-being: Open-vocabulary Text-to-Motion Generation with Wordless Training, Lin et al.</li>
        <li><b>(CVPR 2023)</b> <a href="https://edge-dance.github.io/">EDGE</a>: Editable Dance Generation From Music, Tseng et al.</li>
        <li><b>(CVPR 2023)</b> <a href="https://chenxin.tech/mld">MLD</a>: Executing your Commands via Motion Diffusion in Latent Space, Chen et al.</li>
        <li><b>(CVPR 2023)</b> <a href="https://sigal-raab.github.io/MoDi">MoDi</a>: Unconditional Motion Synthesis from Diverse Data, Raab et al.</li>
        <li><b>(CVPR 2023)</b> <a href="https://vcai.mpi-inf.mpg.de/projects/MoFusion/">MoFusion</a>: A Framework for Denoising-Diffusion-based Motion Synthesis, Dabral et al.</li>
        <li><b>(CVPR 2023)</b> <a href="https://arxiv.org/abs/2303.14926">Mo et al.</a>: Continuous Intermediate Token Learning with Implicit Motion Manifold for Keyframe Based Motion Interpolation, Mo et al.</li>
        <li><b>(ICLR 2023)</b> <a href="https://guytevet.github.io/mdm-page/">HMDM</a>: Human Motion Diffusion Model, Tevet et al.</li>
        <li><b>(TPAMI 2023)</b> <a href="https://mingyuan-zhang.github.io/projects/MotionDiffuse.html">MotionDiffuse</a>: Text-Driven Human Motion Generation with Diffusion Model, Zhang et al.</li>
        <li><b>(TPAMI 2023)</b> <a href="https://www.mmlab-ntu.com/project/bailando/">Bailando++</a>: 3D Dance GPT with Choreographic Memory, Li et al.</li>
        <li><b>(ArXiv 2023)</b> <a href="https://zixiangzhou916.github.io/UDE-2/">UDE-2</a>: A Unified Framework for Multimodal, Multi-Part Human Motion Synthesis, Zhou et al.</li>
        <li><b>(ArXiv 2023)</b> <a href="https://pjyazdian.github.io/MotionScript/">Motion Script</a>: Natural Language Descriptions for Expressive 3D Human Motions, Yazdian et al.</li>
    </ul></details>
    <details>
    <summary><h3>2022 and earlier</h3></summary>
    <ul style="margin-left: 5px;">
        <li><b>(NeurIPS 2022)</b> <a href="https://github.com/c-he/NeMF">NeMF</a>: Neural Motion Fields for Kinematic Animation, He et al.</li>
        <li><b>(SIGGRAPH Asia 2022)</b> <a href="https://github.com/nv-tlabs/PADL">PADL</a>: Language-Directed Physics-Based Character, Juravsky et al.</li>
        <li><b>(SIGGRAPH Asia 2022)</b> <a href="https://pku-mocca.github.io/Rhythmic-Gesticulator-Page/">Rhythmic Gesticulator</a>: Rhythm-Aware Co-Speech Gesture Synthesis with Hierarchical Neural Embeddings, Ao et al.</li>
        <li><b>(3DV 2022)</b> <a href="https://teach.is.tue.mpg.de/">TEACH</a>: Temporal Action Composition for 3D Human, Athanasiou et al.</li>
        <li><b>(ECCV 2022)</b> <a href="https://github.com/PACerv/ImplicitMotion">Implicit Motion</a>: Implicit Neural Representations for Variable Length Human Motion Generation, Cervantes et al.</li>
        <li><b>(ECCV 2022)</b> <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136810707.pdf">Zhong et al.</a>: Learning Uncoupled-Modulation CVAE for 3D Action-Conditioned Human Motion Synthesis, Zhong et al.</li>
        <li><b>(ECCV 2022)</b> <a href="https://guytevet.github.io/motionclip-page/">MotionCLIP</a>: Exposing Human Motion Generation to CLIP Space, Tevet et al.</li>
        <li><b>(ECCV 2022)</b> <a href="https://europe.naverlabs.com/research/computer-vision/posegpt">PoseGPT</a>: Quantizing human motion for large scale generative modeling, Lucas et al.</li>
        <li><b>(ECCV 2022)</b> <a href="https://mathis.petrovich.fr/temos/">TEMOS</a>: Generating diverse human motions from textual descriptions, Petrovich et al.</li>
        <li><b>(ECCV 2022)</b> <a href="https://ericguo5513.github.io/TM2T/">TM2T</a>: Stochastic and Tokenized Modeling for the Reciprocal Generation of 3D Human Motions and Texts, Guo et al.</li>
        <li><b>(SIGGRAPH 2022)</b> <a href="https://hongfz16.github.io/projects/AvatarCLIP.html">AvatarCLIP</a>: Zero-Shot Text-Driven Generation and Animation of 3D Avatars, Hong et al.</li>
        <li><b>(SIGGRAPH 2022)</b> <a href="https://dl.acm.org/doi/10.1145/3528223.3530178">DeepPhase</a>: Periodic autoencoders for learning motion phase manifolds, Starke et al.</li>
        <li><b>(CVPR 2022)</b> <a href="https://ericguo5513.github.io/text-to-motion">Guo et al.</a>: Generating Diverse and Natural 3D Human Motions from Text, Guo et al.</li>
        <li><b>(CVPR 2022)</b> <a href="https://www.mmlab-ntu.com/project/bailando/">Bailando</a>: 3D Dance Generation by Actor-Critic GPT with Choreographic Memory, Li et al.</li>
        <li><b>(ICCV 2021)</b> <a href="https://mathis.petrovich.fr/actor/index.html">ACTOR</a>: Action-Conditioned 3D Human Motion Synthesis with Transformer VAE, Petrovich et al.</li>
        <li><b>(ICCV 2021)</b> <a href="https://google.github.io/aichoreographer/">AIST++</a>: AI Choreographer: Music Conditioned 3D Dance Generation with AIST++, Li et al.</li>
        <li><b>(SIGGRAPH 2021)</b> <a href="https://dl.acm.org/doi/10.1145/3450626.3459881">Starke et al.</a>: Neural animation layering for synthesizing martial arts movements, Starke et al.</li>
        <li><b>(CVPR 2021)</b> <a href="https://yz-cnsdqz.github.io/eigenmotion/MOJO/index.html">MOJO</a>: We are More than Our Joints: Predicting how 3D Bodies Move, Zhang et al.</li>
        <li><b>(ECCV 2020)</b> <a href="https://www.ye-yuan.com/dlow">DLow</a>: Diversifying Latent Flows for Diverse Human Motion Prediction, Yuan et al.</li>
        <li><b>(SIGGRAPH 2020)</b> <a href="https://www.ipab.inf.ed.ac.uk/cgvu/basketball.pdf">Starke et al.</a>: Local motion phases for learning multi-contact character movements, Starke et al.</li>
    </ul></details>
</ul></details>

<span id="hhi"></span>
<details open>
<summary><h2>Multi-Person Interaction Generation</h2></summary>
<ul style="margin-left: 5px;">
    <li><b>(SIGGRAPH 2025)</b> <a href="https://arxiv.org/pdf/2505.17860">Xu et al</a>: Multi-Person Interaction Generation from Two-Person Motion Priors, Xu et al.</li>
    <li><b>(CVPR 2025)</b> <a href="https://aigc-explorer.github.io/TIMotion-page/">TIMotion</a>: Temporal and Interactive Framework for Efficient Human-Human Motion Generation, Wang et al.</li>
    <li><b>(ICLR 2025)</b> <a href="https://openreview.net/forum?id=UxzKcIZedp">Think Then React</a>: Towards Unconstrained Action-to-Reaction Motion Generation, Tan et al.</li>
    <li><b>(ICLR 2025)</b> <a href="https://zju3dv.github.io/ready_to_react/">Ready-to-React</a>: Online Reaction Policy for Two-Character Interaction Generation, Cen et al.</li>
    <li><b>(ICLR 2025)</b> <a href="https://gohar-malik.github.io/intermask">InterMask</a>: 3D Human Interaction Generation via Collaborative Masked Modelling, Javed et al.</li>
    <li><b>(3DV 2025)</b> <a href="https://arxiv.org/abs/2312.08983">Interactive Humanoid</a>: Online Full-Body Motion Reaction Synthesis with Social Affordance Canonicalization and Forecasting, Liu et al.</li>
    <li><b>(ArXiv 2025)</b> <a href="https://yw0208.github.io/physiinter/">PhysInter</a>: Integrating Physical Mapping for High-Fidelity Human Interaction Generation, Yao et al.</li>
    <li><b>(ArXiv 2025)</b> <a href="https://arxiv.org/pdf/2506.03084">InterMamba</a>: Efficient Human-Human Interaction Generation with Adaptive Spatio-Temporal Mamba, Wu et al.</li>
    <li><b>(ArXiv 2025)</b> <a href="https://arxiv.org/pdf/2505.11334">MARRS</a>: MaskedAutoregressive Unit-based Reaction Synthesis, Wang et al.</li>
    <li><b>(ArXiv 2025)</b> <a href="https://socialgenx.github.io/">SocialGen</a>: Modeling Multi-Human Social Interaction with Language Models, Yu et al.</li>
    <li><b>(ArXiv 2025)</b> <a href="https://arflow2025.github.io/">ARFlow</a>: Human Action-Reaction Flow Matching with Physical Guidance, Jiang et al.</li>
    <li><b>(ArXiv 2025)</b> <a href="https://arxiv.org/pdf/2503.13120">Fan et al</a>: 3D Human Interaction Generation: A Survey, Fan et al.</li>
    <li><b>(ArXiv 2025)</b> <a href="https://arxiv.org/pdf/2503.04816">Invisible Strings</a>: Invisible Strings: Revealing Latent Dancer-to-Dancer Interactions with Graph Neural Networks, Zerkowski et al. </li>
    <li><b>(ArXiv 2025)</b> <a href="https://arxiv.org/pdf/2502.11563">Leader and Follower</a>: Interactive Motion Generation under Trajectory Constraints, Wang et al. </li>
    <li><b>(ArXiv 2024)</b> <a href="https://arxiv.org/pdf/2412.16670">Two in One</a>: Unified Multi-Person Interactive Motion Generation by Latent Diffusion Transformer, Li et al.</li>
    <li><b>(ArXiv 2024)</b> <a href="https://arxiv.org/pdf/2412.02419">It Takes Two</a>: Real-time Co-Speech Two-person’s Interaction Generation via Reactive Auto-regressive Diffusion Model, Shi et al.</li>
    <li><b>(ArXiv 2024)</b> <a href="https://arxiv.org/abs/2409.20502">COLLAGE</a>: Collaborative Human-Agent Interaction Generation using Hierarchical Latent Diffusion and Language Models, Daiya et al.</li>
    <li><b>(NeurIPS 2024)</b> <a href="https://jyuntins.github.io/harmony4d/">Harmony4D</a>: A Video Dataset for In-The-Wild Close Human Interactions, Khirodkar et al.</li>
    <li><b>(NeurIPS 2024)</b> <a href="https://github.com/zhenzhiwang/intercontrol">InterControl</a>: Generate Human Motion Interactions by Controlling Every Joint, Wang et al.</li>
    <li><b>(ACM MM 2024)</b> <a href="https://yunzeliu.github.io/PhysReaction/">PhysReaction</a>: Physically Plausible Real-Time Humanoid Reaction Synthesis via Forward Dynamics Guided 4D Imitation, Liu et al.</li>
    <li><b>(ECCV 2024)</b> <a href="https://arxiv.org/abs/2405.18483">Shan et al</a>: Towards Open Domain Text-Driven Synthesis of Multi-Person Motions, Shan et al.</li>
    <li><b>(ECCV 2024)</b> <a href="https://vcai.mpi-inf.mpg.de/projects/remos/">ReMoS</a>: 3D Motion-Conditioned Reaction Synthesis for Two-Person Interactions, Ghosh et al.</li>
    <li><b>(CVPR 2024)</b> <a href="https://liangxuy.github.io/inter-x/">Inter-X</a>: Towards Versatile Human-Human Interaction Analysis, Xu et al.</li>
    <li><b>(CVPR 2024)</b> <a href="https://github.com/liangxuy/ReGenNet">ReGenNet</a>: Towards Human Action-Reaction Synthesis, Xu et al.</li>
    <li><b>(CVPR 2024)</b> <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Fang_Capturing_Closely_Interacted_Two-Person_Motions_with_Reaction_Priors_CVPR_2024_paper.pdf">Fang et al.</a>: Capturing Closely Interacted Two-Person Motions with Reaction Priors, Fan et al.</li>
    <li><b>(CVPR Workshop 2024)</b> <a href="https://openaccess.thecvf.com/content/CVPR2024W/HuMoGen/html/Ruiz-Ponce_in2IN_Leveraging_Individual_Information_to_Generate_Human_INteractions_CVPRW_2024_paper.html">in2IN</a>: in2IN: Leveraging Individual Information to Generate Human INteractions, Ruiz-Ponce et al.</li>
    <li><b>(IJCV 2024)</b> <a href="https://tr3e.github.io/intergen-page/">InterGen</a>: Diffusion-based Multi-human Motion Generation under Complex Interactions, Liang et al.</li>
    <li><b>(ICCV 2023)</b> <a href="https://liangxuy.github.io/actformer/">ActFormer</a>: A GAN-based Transformer towards General Action-Conditioned 3D Human Motion Generation, Xu et al.</li>
    <li><b>(ICCV 2023)</b> <a href="https://github.com/line/Human-Interaction-Generation">Tanaka et al.</a>: Role-aware Interaction Generation from Textual Description, Tanaka et al.</li>
    <li><b>(CVPR 2023)</b> <a href="https://yifeiyin04.github.io/Hi4D/">Hi4D</a>: 4D Instance Segmentation of Close Human Interaction, Yin et al.</li>
    <li><b>(CVPR 2022)</b> <a href="https://github.com/GUO-W/MultiMotion">ExPI</a>: Multi-Person Extreme Motion Prediction, Guo et al.</li>
    <li><b>(CVPR 2020)</b> <a href="https://ci3d.imar.ro/home">CHI3D</a>: Three-Dimensional Reconstruction of Human Interactions, Fieraru et al.</li>
</ul></details>
